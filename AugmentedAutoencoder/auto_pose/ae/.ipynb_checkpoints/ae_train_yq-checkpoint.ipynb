{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/opt/ros/melodic/lib/python2.7/dist-packages', '/home/yq/anaconda3/envs/aae/lib/python27.zip', '/home/yq/anaconda3/envs/aae/lib/python2.7', '/home/yq/anaconda3/envs/aae/lib/python2.7/plat-linux2', '/home/yq/anaconda3/envs/aae/lib/python2.7/lib-tk', '/home/yq/anaconda3/envs/aae/lib/python2.7/lib-old', '/home/yq/anaconda3/envs/aae/lib/python2.7/lib-dynload', '/home/yq/.local/lib/python2.7/site-packages', '/home/yq/anaconda3/envs/aae/lib/python2.7/site-packages', '/home/yq/anaconda3/envs/aae/lib/python2.7/site-packages', '/home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/IPython/extensions', '/home/yq/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ae_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # -*- coding: utf-8 -*-\n",
    "import os\n",
    "import configparser\n",
    "import argparse\n",
    "import numpy as np\n",
    "import signal\n",
    "import shutil\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import progressbar\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import package moduel**\n",
    "Can not use 'from . import ae_factory as factory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_pose.ae import ae_factory as factory\n",
    "from auto_pose.ae import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exp_group', 'my_autoencoder']\n"
     ]
    }
   ],
   "source": [
    "workspace_path = os.environ.get('AE_WORKSPACE_PATH')\n",
    "full_name = \"exp_group/my_autoencoder\".split('/')\n",
    "print(full_name)\n",
    "experiment_name = full_name.pop()\n",
    "experiment_group = full_name.pop() if len(full_name) > 0 else ''\n",
    "\n",
    "debug_mode = True\n",
    "generate_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cfg_file_path', '/home/yq/Data/23_pose_estimation/aae/autoencode_ws/cfg/exp_group/my_autoencoder.cfg')\n",
      "('log_dir', '/home/yq/Data/23_pose_estimation/aae/autoencode_ws/experiments/exp_group/my_autoencoder')\n",
      "('checkpoint_file', '/home/yq/Data/23_pose_estimation/aae/autoencode_ws/experiments/exp_group/my_autoencoder/checkpoints/chkpt')\n",
      "('ckpt_dir', '/home/yq/Data/23_pose_estimation/aae/autoencode_ws/experiments/exp_group/my_autoencoder/checkpoints')\n",
      "('train_fig_dir', '/home/yq/Data/23_pose_estimation/aae/autoencode_ws/experiments/exp_group/my_autoencoder/train_figures')\n",
      "('dataset_path', '/home/yq/Data/23_pose_estimation/aae/autoencode_ws/tmp_datasets')\n"
     ]
    }
   ],
   "source": [
    "cfg_file_path = u.get_config_file_path(workspace_path, experiment_name, experiment_group)\n",
    "log_dir = u.get_log_dir(workspace_path, experiment_name, experiment_group)\n",
    "checkpoint_file = u.get_checkpoint_basefilename(log_dir)\n",
    "ckpt_dir = u.get_checkpoint_dir(log_dir)\n",
    "train_fig_dir = u.get_train_fig_dir(log_dir)\n",
    "dataset_path = u.get_dataset_path(workspace_path)\n",
    "print(\"cfg_file_path\", cfg_file_path)\n",
    "print(\"log_dir\", log_dir)\n",
    "print(\"checkpoint_file\", checkpoint_file)\n",
    "print(\"ckpt_dir\", ckpt_dir)\n",
    "print(\"train_fig_dir\", train_fig_dir)\n",
    "print(\"dataset_path\", dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "if not os.path.exists(train_fig_dir):\n",
    "    os.makedirs(train_fig_dir)\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'model', u'cad'), (u'h', u'128'), (u'w', u'128'), (u'c', u'3'), (u'radius', u'700'), (u'render_dims', u'(720, 540)'), (u'k', u'[1075.65, 0, 720/2, 0, 1073.90, 540/2, 0, 0, 1]'), (u'vertex_scale', u'1'), (u'antialiasing', u'1'), (u'pad_factor', u'1.2'), (u'clip_near', u'10'), (u'clip_far', u'10000'), (u'noof_training_imgs', u'1000'), (u'noof_bg_imgs', u'1000')]\n"
     ]
    }
   ],
   "source": [
    "args = configparser.ConfigParser()\n",
    "args.read(cfg_file_path)\n",
    "print(args.items('Dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yq/Data/23_pose_estimation/aae/autoencode_ws/tmp_datasets/854da3ed842cbe1456db99775ee087c5.npz\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "current_config_hash = hashlib.md5((str(args.items('Dataset')+args.items('Paths'))).encode('utf-8')).hexdigest()\n",
    "current_file_name = os.path.join(dataset_path, current_config_hash + '.npz')\n",
    "print(current_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2(cfg_file_path, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/queue.py:24: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/queue.py:28: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/encoder.py:46: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/encoder.py:63: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "(128, 128, 3)\n",
      "[[8, 8], [16, 16], [32, 32], [64, 64]]\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/decoder.py:51: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "(?, 128, 128, 3)\n",
      "(?, 128, 128, 3)\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/decoder.py:95: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/decoder.py:98: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/decoder.py:133: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/ae.py:48: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yq/anaconda3/envs/aae/lib/python2.7/site-packages/auto_pose/ae/codebook.py:32: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(experiment_name):\n",
    "    dataset = factory.build_dataset(dataset_path, args)\n",
    "    queue = factory.build_queue(dataset, args)\n",
    "    encoder = factory.build_encoder(queue.x, args, is_training=True)\n",
    "    decoder = factory.build_decoder(queue.y, encoder, args, is_training=True)\n",
    "    ae = factory.build_ae(encoder, decoder, args)\n",
    "    codebook = factory.build_codebook(encoder, dataset, args)\n",
    "    train_op = factory.build_train_op(ae, args)\n",
    "    saver = tf.train.Saver(save_relative_paths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "5000\n",
      "cad\n"
     ]
    }
   ],
   "source": [
    "num_iter = args.getint('Training', 'NUM_ITER') if not debug_mode else 100000\n",
    "save_interval = args.getint('Training', 'SAVE_INTERVAL')\n",
    "model_type = args.get('Dataset', 'MODEL')\n",
    "print(num_iter)\n",
    "print(save_interval)\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type=='dsprites':\n",
    "    dataset.get_sprite_training_images(args)\n",
    "else:\n",
    "    dataset.get_training_images(dataset_path, args)\n",
    "    dataset.load_bg_images(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'code': u'Sequential([\\nSometimes(0.5, Affine(scale=(1.0, 1.2))),\\nSometimes(0.5, CoarseDropout( p=0.2, size_percent=0.05) ),\\nSometimes(0.5, GaussianBlur(1.2*np.random.rand())),\\nSometimes(0.5, Add((-25, 25), per_channel=0.3)),\\nSometimes(0.3, Invert(0.2, per_channel=True)),\\nSometimes(0.5, Multiply((0.6, 1.4), per_channel=0.5)),\\nSometimes(0.5, Multiply((0.6, 1.4))),\\nSometimes(0.5, ContrastNormalization((0.5, 2.2), per_channel=0.3))\\n], random_order=False)', u'clip_near': u'10', u'clip_far': u'10000', u'radius': u'700', u'realistic_occlusion': u'False', u'min_n_views': u'2562', u'num_cyclo': u'36', u'embed_bb': u'True', u'background_images_glob': u'/home/yq/Data/23_pose_estimation/aae/tello_background_images/*.jpg', u'noof_training_imgs': u'1000', u'pad_factor': u'1.2', u'square_occlusion': u'False', u'queue_size': u'50', u'num_threads': u'10', u'c': u'3', u'vertex_scale': u'1', u'antialiasing': u'1', u'h': u'128', u'k': u'[1075.65, 0, 720/2, 0, 1073.90, 540/2, 0, 0, 1]', u'max_rel_offset': u'0.20', u'model_path': u'/home/yq/Data/23_pose_estimation/dji_tello_cad_model/correct_size_centered_triangles/correct_size_centered_triangles_1000times_3.ply', u'w': u'128', u'render_dims': u'(720, 540)', u'model': u'cad', u'noof_bg_imgs': u'1000'}\n"
     ]
    }
   ],
   "source": [
    "# def build_dataset(dataset_path, args):\n",
    "dataset_args = { k:v for k,v in\n",
    "               args.items('Dataset') +\n",
    "               args.items('Paths') +\n",
    "               args.items('Augmentation')+\n",
    "               args.items('Queue') +\n",
    "               args.items('Embedding')}\n",
    "print(dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yq/Data/23_pose_estimation/aae/autoencode_ws/tmp_datasets\n"
     ]
    }
   ],
   "source": [
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from auto_pose.ae.dataset import Dataset\n",
    "# dataset = Dataset(dataset_path, **dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import hashlib\n",
    "import glob\n",
    "import os\n",
    "import progressbar\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_pose.ae.pysixd_stuff import transform\n",
    "from auto_pose.ae.pysixd_stuff import view_sampler\n",
    "from auto_pose.ae.utils import lazy_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3)\n",
      "1000\n",
      "/home/yq/Data/23_pose_estimation/aae/autoencode_ws/tmp_datasets\n",
      "1000\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "class Dataset(object):\n",
    "    \n",
    "    def __init__(self, dataset_path, **kw):\n",
    "        self.shape = (int(kw['h']), int(kw['w']), int(kw['c']))\n",
    "        self.noof_training_imgs = int(kw['noof_training_imgs'])\n",
    "        self.dataset_path = dataset_path\n",
    "        \n",
    "        self.bg_img_paths = glob.glob(kw['background_images_glob'])\n",
    "        self.noof_bg_imgs = min(int(kw['noof_bg_imgs']), len(self.bg_img_paths))\n",
    "        \n",
    "        self._kw = kw\n",
    "        # self._aug = eval(self._kw['code'])\n",
    "\n",
    "        self.train_x = np.empty( (self.noof_training_imgs,) + self.shape, dtype=np.uint8 )\n",
    "        self.mask_x = np.empty( (self.noof_training_imgs,) + self.shape[:2], dtype= bool)\n",
    "        self.noof_obj_pixels = np.empty( (self.noof_training_imgs,), dtype= bool)\n",
    "        self.train_y = np.empty( (self.noof_training_imgs,) + self.shape, dtype=np.uint8 )\n",
    "        self.bg_imgs = np.empty( (self.noof_bg_imgs,) + self.shape, dtype=np.uint8 )\n",
    "        if np.float(eval(self._kw['realistic_occlusion'])):\n",
    "            self.random_syn_masks\n",
    "            \n",
    "    def get_training_images(self, dataset_path, args):\n",
    "        current_config_hash = hashlib.md5((str(args.items('Dataset')+args.items('Paths'))).encode('utf-8')).hexdigest()\n",
    "        current_file_name = os.path.join(dataset_path, current_config_hash + '.npz')\n",
    "\n",
    "        if os.path.exists(current_file_name):\n",
    "            training_data = np.load(current_file_name)\n",
    "            self.train_x = training_data['train_x'].astype(np.uint8)\n",
    "            self.mask_x = training_data['mask_x']\n",
    "            self.train_y = training_data['train_y'].astype(np.uint8)\n",
    "        else:\n",
    "            self.render_training_images()\n",
    "            np.savez(current_file_name, train_x = self.train_x, mask_x = self.mask_x, train_y = self.train_y)\n",
    "        self.noof_obj_pixels = np.count_nonzero(self.mask_x==0,axis=(1,2))\n",
    "        print('loaded %s training images' % len(self.train_x))\n",
    "        \n",
    "    @lazy_property\n",
    "    def random_syn_masks(self):\n",
    "        import bitarray\n",
    "        workspace_path = os.environ.get('AE_WORKSPACE_PATH')\n",
    "\n",
    "        random_syn_masks = bitarray.bitarray()\n",
    "        with open(os.path.join(workspace_path,'random_tless_masks/arbitrary_syn_masks_1000.bin'), 'r') as fh:\n",
    "            random_syn_masks.fromfile(fh)\n",
    "        occlusion_masks = np.fromstring(random_syn_masks.unpack(), dtype=np.bool)\n",
    "        occlusion_masks = occlusion_masks.reshape(-1,224,224,1).astype(np.float32)\n",
    "        print(occlusion_masks.shape)\n",
    "\n",
    "        occlusion_masks = np.array([cv2.resize(mask,(self.shape[0],self.shape[1]), interpolation = cv2.INTER_NEAREST) for mask in occlusion_masks])\n",
    "        return occlusion_masks\n",
    "    \n",
    "    @lazy_property\n",
    "    def renderer(self):\n",
    "        from auto_pose.meshrenderer import meshrenderer, meshrenderer_phong\n",
    "        if self._kw['model'] == 'cad':\n",
    "            renderer = meshrenderer.Renderer(\n",
    "               [self._kw['model_path']],\n",
    "               int(self._kw['antialiasing']),\n",
    "               self.dataset_path,\n",
    "               float(self._kw['vertex_scale'])\n",
    "            )\n",
    "        elif self._kw['model'] == 'reconst':\n",
    "            renderer = meshrenderer_phong.Renderer(\n",
    "               [self._kw['model_path']],\n",
    "               int(self._kw['antialiasing']),\n",
    "               self.dataset_path,\n",
    "               float(self._kw['vertex_scale'])\n",
    "            )\n",
    "        else:\n",
    "            'Error: neither cad nor reconst in model path!'\n",
    "            exit()\n",
    "        return renderer\n",
    "    \n",
    "    def render_training_images(self):\n",
    "        kw = self._kw\n",
    "        H, W = int(kw['h']), int(kw['w'])\n",
    "        render_dims = eval(kw['render_dims'])\n",
    "        K = eval(kw['k'])\n",
    "        K = np.array(K).reshape(3,3)\n",
    "        clip_near = float(kw['clip_near'])\n",
    "        clip_far = float(kw['clip_far'])\n",
    "        pad_factor = float(kw['pad_factor'])\n",
    "        max_rel_offset = float(kw['max_rel_offset'])\n",
    "        t = np.array([0, 0, float(kw['radius'])])\n",
    "\n",
    "        widgets = ['Training: ', progressbar.Percentage(),\n",
    "             ' ', progressbar.Bar(),\n",
    "             ' ', progressbar.Counter(), ' / %s' % self.noof_training_imgs,\n",
    "             ' ', progressbar.ETA(), ' ']\n",
    "        bar = progressbar.ProgressBar(maxval=self.noof_training_imgs,widgets=widgets)\n",
    "        bar.start()\n",
    "\n",
    "        for i in np.arange(self.noof_training_imgs):\n",
    "            bar.update(i)\n",
    "\n",
    "            # print '%s/%s' % (i,self.noof_training_imgs)\n",
    "            # start_time = time.time()\n",
    "            R = transform.random_rotation_matrix()[:3,:3]\n",
    "            bgr_x, depth_x = self.renderer.render(\n",
    "                obj_id=0,\n",
    "                W=render_dims[0],\n",
    "                H=render_dims[1],\n",
    "                K=K.copy(),\n",
    "                R=R,\n",
    "                t=t,\n",
    "                near=clip_near,\n",
    "                far=clip_far,\n",
    "                random_light=True\n",
    "            )\n",
    "            bgr_y, depth_y = self.renderer.render(\n",
    "                obj_id=0,\n",
    "                W=render_dims[0],\n",
    "                H=render_dims[1],\n",
    "                K=K.copy(),\n",
    "                R=R,\n",
    "                t=t,\n",
    "                near=clip_near,\n",
    "                far=clip_far,\n",
    "                random_light=False\n",
    "            )\n",
    "            # render_time = time.time() - start_time\n",
    "            # cv2.imshow('bgr_x',bgr_x)\n",
    "            # cv2.imshow('bgr_y',bgr_y)\n",
    "            # cv2.waitKey(0)\n",
    "\n",
    "            ys, xs = np.nonzero(depth_x > 0)\n",
    "\n",
    "            try:\n",
    "                obj_bb = view_sampler.calc_2d_bbox(xs, ys, render_dims)\n",
    "            except ValueError as e:\n",
    "                print('Object in Rendering not visible. Have you scaled the vertices to mm?')\n",
    "                break\n",
    "\n",
    "\n",
    "            x, y, w, h = obj_bb\n",
    "\n",
    "            rand_trans_x = np.random.uniform(-max_rel_offset, max_rel_offset) * w\n",
    "            rand_trans_y = np.random.uniform(-max_rel_offset, max_rel_offset) * h\n",
    "\n",
    "            obj_bb_off = obj_bb + np.array([rand_trans_x,rand_trans_y,0,0])\n",
    "\n",
    "            bgr_x = self.extract_square_patch(bgr_x, obj_bb_off, pad_factor,resize=(W,H),interpolation = cv2.INTER_NEAREST)\n",
    "            depth_x = self.extract_square_patch(depth_x, obj_bb_off, pad_factor,resize=(W,H),interpolation = cv2.INTER_NEAREST)\n",
    "            mask_x = depth_x == 0.\n",
    "\n",
    "\n",
    "            ys, xs = np.nonzero(depth_y > 0)\n",
    "            obj_bb = view_sampler.calc_2d_bbox(xs, ys, render_dims)\n",
    "\n",
    "            bgr_y = self.extract_square_patch(bgr_y, obj_bb, pad_factor,resize=(W,H),interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "            if self.shape[2] == 1:\n",
    "                bgr_x = cv2.cvtColor(np.uint8(bgr_x), cv2.COLOR_BGR2GRAY)[:,:,np.newaxis]\n",
    "                bgr_y = cv2.cvtColor(np.uint8(bgr_y), cv2.COLOR_BGR2GRAY)[:,:,np.newaxis]\n",
    "\n",
    "            self.train_x[i] = bgr_x.astype(np.uint8)\n",
    "            self.mask_x[i] = mask_x\n",
    "            self.train_y[i] = bgr_y.astype(np.uint8)\n",
    "\n",
    "            #print 'rendertime ', render_time, 'processing ', time.time() - start_time\n",
    "        bar.finish()\n",
    "            \n",
    "dataset = Dataset(dataset_path, **dataset_args)\n",
    "print(dataset.shape)\n",
    "print(dataset.noof_training_imgs)\n",
    "print(dataset.dataset_path)\n",
    "# print(dataset.bg_img_paths)\n",
    "print(dataset.noof_bg_imgs)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_checkpoint_path: \"/home/yq/Data/23_pose_estimation/aae/autoencode_ws/experiments/exp_group/my_autoencoder/checkpoints/chkpt-25000\"\n",
      "all_model_checkpoint_paths: \"/home/yq/Data/23_pose_estimation/aae/autoencode_ws/experiments/exp_group/my_autoencoder/checkpoints/chkpt-25000\"\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/yq/Data/23_pose_estimation/aae/autoencode_ws/experiments/exp_group/my_autoencoder/checkpoints/chkpt-25000\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction = 0.9)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "with tf.Session(config=config) as sess:\n",
    "        chkpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "        print(chkpt)\n",
    "        if chkpt and chkpt.model_checkpoint_path:\n",
    "            saver.restore(sess, chkpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
